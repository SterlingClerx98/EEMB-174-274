---
title: "Mar 1, Binomial Regression"
author: "Stephen R. Proulx"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rethinking)
source("../Resources/helper.R")
```

# Today's objectives:  
* logistic regression
* binomial regression
* Calculating contrasts from posterior parameter distributions
* Specifying models with contrasts built in.


## The logistic function

The logit transformation maps numbers between 0 and 1 to numbers between -inf and inf. We will talk about mapping from the probabiliy scale to the logit scale, and vice-versa.
```{r}
logit_dat <- tibble( x= seq(from=0,to=1,by=0.001)) %>%
  mutate(y=logit(x))

ggplot(logit_dat, aes(x=x,y=y))+geom_line()
```


The inverse logit transformation maps numbers between between -inf and inf to numbers between 0 and 1. In our additive models we will be combining effects, how do these translate back to the probability scale?
```{r}
inv_logit_dat <- tibble( x= seq(from=-10,to=10,by=0.1)) %>%
  mutate(y=inv_logit(x) , y_add=inv_logit(x+0.5) , y_diff=y_add-y)

ggplot(inv_logit_dat, aes(x=x,y=y))+
  geom_line(color="black")+
  geom_line(aes(x=x,y=y_add) , color="red")

ggplot(inv_logit_dat, aes(x=x,y=y_diff))+
  geom_line(color="black")

```

The effect of our additive model on the probability scale is always dependent on both values that we are combining. This is why McElreath talks about interactions arising even when you don't build them in.   

## Logistic chimps
 
Load the data and ad a column for coded treatment type.
```{r}
## R code 11.1
library(rethinking)
data(chimpanzees)
d <- chimpanzees

## R code 11.2
d<- mutate(d , treatment = 1 + prosoc_left + 2*condition)

```

```{r}
view(d)
```




We can fit the probability of pulling the left lever for all trials without any additional predictors. They more often pull right.
```{r}

## R code 11.4
m11.1 <- quap(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a ,
        a ~ dnorm( 0 , 1.5)
    ) , data=d )

precis(m11.1)
```

How's that prior looking? Not good until you turn the sd down to about 1.5!
```{r}
## R code 11.5
set.seed(1999)
prior <- extract.prior( m11.1 , n=1e6 ) %>% 
  as_tibble %>%
  mutate(p=inv_logit(a))
  
## R code 11.6
ggplot(prior, aes(x=p)) + geom_density()
```

Check the prior for the model with treatment effects. To recover the modeled values in the binomial likelihood we have to recreate the values for each treatment. If we turn down the variance for `b` we get a more reasonable prior on the probability scale.
```{r}
## R code 11.7
m11.2 <- quap(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a + b[treatment] ,
        a ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 0.5)
    ) , data=d )

set.seed(1999)
prior <- extract.prior( m11.2 , n=1e6 ) %>% 
  as_tibble() %>%
  mutate(p1=inv_logit(a+ b[,1]),
         p2=inv_logit(a+ b[,2]),
         p3=inv_logit(a+ b[,3]),
         p4=inv_logit(a+ b[,4]),
         delta_prop12=p2-p1)

## R code 11.8
ggplot(prior, aes(x=abs(delta_prop12))) + geom_density()
```


Now we'll perform MCMC sampling on the model that includes chimp effects as well as treatment effects.

```{r}
## R code 11.10
# trimmed data list
dat_list=select(d, pulled_left,actor,treatment)

## R code 11.11
m11.4 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + b[treatment] ,
        a[actor] ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 0.5 )
    ) , data=dat_list , chains=4 , log_lik=FALSE )
```


And the parameter summary from the posterior:
```{r}
precis( m11.4 , depth=2 )
```


We will want to use just the samples, the object `m11.4` contains a lot of other (sometimes useful) things, so here we'll just extract the samples. The function `extract` is part of the `rstan` package, so it produces a dataframe in a format that is usable by other packages based on `rstan`. (Some `rethinking` package functions won't work when we extract this way.)

So we extract and then convert back to the probability scale. Even though we performed our model fitting on the logit scale, we can do all the same sorts of analyses on the transformed parameter. THIS IS A VERY USEFUL THING TO BE ABLE TO DO!
```{r}
## similar to R code 11.12
post <- extract(m11.4@stanfit) %>% 
  as_tibble() %>%
  mutate(p_left = inv_logit(a))

```

```{r}
view(post)
```


We can visualize the parameters themselves, starting with the chimp-specific "intercepts". We can plot them on the logit scale or back on the probability scale. Notice how the logit transformation stretches values that are scrunched near the edges of the probability scale.
```{r}
bayesplot::mcmc_intervals( select(post, a ) )


bayesplot::mcmc_intervals( select(post, a ) %>% mutate_all(inv_logit) )

```

Similarly for the slope values, b.
```{r}
bayesplot::mcmc_intervals( select(post, b ) )


bayesplot::mcmc_intervals( select(post, b ) %>% mutate_all(inv_logit) )

```

And we can start to think of the contrasts that we are interested in making
```{r}
post<- mutate(post, deltab13 = b[,1]-b[,3], deltab24 = b[,2]-b[,4])

bayesplot::mcmc_intervals( select(post, deltab13,deltab24 ) )

```







Posterior plot: This doesn't have the nice labels as in the book. 
```{r}
## R code 11.17
dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link( m11.4 , data=dat ) %>% 
  as_tibble() %>%
  gather("column.name","value",1:28) %>%
  separate(column.name,c("V","index"),sep=1) %>%
  mutate(index=as.numeric(index))%>%
  select(-V)%>%
  group_by(index)%>%
  summarise(
    mean_value=mean(value),
    lower_value=quantile(value,0.1),
    upper_value=quantile(value,0.9)) %>%
  ungroup()


ggplot(p_post,aes(x=index, y= mean_value)) + 
  geom_point() +
  geom_errorbar(aes(ymin=lower_value,ymax=upper_value))+
  ylim(0,1)
```


### comparing models without an interaction 
This model assigned independent parameters to each of our 4 treatments. But really these are two pairs of binary treatments, food on left/right and presence/absence of another chimp. 
Re-run the analysis of model `m11.4` and include the `log-lik=TRUE` and compare it to a model with additive effects of food location and chimp partner. Also think about this model really does. 



 
## Binomial admissions

```{r}
## R code 11.28
library(rethinking)
data(UCBadmit)
d <- UCBadmit %>% 
  as_tibble() %>%
  mutate(gid= (applicant.gender=="male")*1+(applicant.gender=="female")*2,
         dept_id=as.integer( as.factor(dept)))
```


```{r}
## R code 11.29ish
m11.7 <- ulam(
    alist(
        admit ~ dbinom( applications , p ) ,
        logit(p) <- a[gid] ,
        a[gid] ~ dnorm( 0 , 1.5 )
    ) , data=select(d,admit,applications,gid) , chains=4 )
precis( m11.7 , depth=2 )
```


```{r}
## R code 11.30
post <- extract.samples(m11.7) %>% as_tibble()%>%
  mutate(diff_a=a[,1]-a[,2],
         diff_p=inv_logit(a[,1])-inv_logit(a[,2]))
mcmc_intervals(select(post,diff_a,diff_p)) 
```


### Posterior plots
Use `sim` to generate simulated data from the posterior distribution and choose some ways to visualize and compare to the actual data.


### Alternative formulations of the same model.

Re-run m11.8 but include the likelihood scores so we can get WAIC values
```{r}
m11.8 <- ulam(
    alist(
        admit ~ dbinom( applications , p ) ,
        logit(p) <- a[gid] + delta[dept_id] ,
        a[gid] ~ dnorm( 0 , 1.5 ) ,
        delta[dept_id] ~ dnorm( 0 , 1.5 )
    ) , data=select(d,admit,applications,gid,dept_id) , cores=4, chains=4 , iter=4000,
    log_lik = TRUE) 
```


The alternate model where we force the `a` values to be a difference from an intercept. 
```{r}
m11.8A <- ulam(
    alist(
        admit ~ dbinom( applications , p ) ,
        logit(p) <- a*(gid-1)  + delta[dept_id] ,
        a ~ dnorm( 0 , 0.1 ) ,
        delta[dept_id] ~ dnorm( 0 , 1.5 )
    ) , data=select(d,admit,applications,gid,dept_id) , cores=4, chains=4 , iter=4000,
    log_lik = TRUE) 
```

Compare the intervals and standard deviations of the parameters in the two models, what do you notice?
```{r}
precis(m11.8, depth=2)
precis(m11.8A, depth=2)
```

Now compare the WAIC of the two models.
```{r}
compare(m11.8,m11.8A)
```


```{r}
## R code 11.33
post <- extract.samples(m11.8A)%>%
  as_tibble() %>%
  mutate(odds_p = exp(a))

precis( select(post,a,odds_p))
```


```{r}
## R code 11.33
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```


